# Sneaker Collector's Assistant - Architecture Diagrams

## System Architecture (High-Level)

```mermaid
graph TB
    subgraph "User Layer"
        A[ğŸ‘¤ User Input<br/>Apify Console]
        B[â° Scheduled Runs<br/>Apify Scheduler]
        C[ğŸ”Œ API Calls<br/>External Integration]
    end

    subgraph "Sneaker Collector's Assistant - Main Actor"
        D[ğŸ“¥ Input Parser<br/>& Validator]
        E[ğŸ“‹ Watchlist<br/>Manager]
        F[ğŸ¯ Marketplace<br/>Orchestrator]

        subgraph "Marketplace Scrapers"
            G1[ğŸª StockX<br/>Scraper]
            G2[ğŸª GOAT<br/>Scraper]
            G3[ğŸª eBay<br/>Scraper]
            G4[ğŸª Facebook<br/>Marketplace]
            G5[ğŸª Craigslist<br/>Scraper]
            G6[ğŸª OfferUp<br/>Scraper]
        end

        H[ğŸ”„ Data<br/>Aggregator]
        I[âœ¨ Data Normalizer<br/>& Enricher]
        J[ğŸ” Deduplication<br/>Engine]
        K[ğŸ“Š Historical Data<br/>Manager]
        L[ğŸš¨ Alert Condition<br/>Evaluator]
        M[ğŸ“¬ Notification<br/>Dispatcher]
    end

    subgraph "Storage Layer (Apify)"
        N[(ğŸ“¦ Dataset<br/>Current Listings)]
        O[(ğŸ“ˆ Dataset<br/>Historical Snapshots)]
        P[(ğŸ”‘ KV Store<br/>User Watchlists)]
        Q[(âœ… KV Store<br/>Seen Listings)]
    end

    subgraph "Notification Layer"
        R[âœ‰ï¸ Email<br/>SendGrid API]
        S[ğŸ’¬ Slack<br/>Webhook]
        T[ğŸ“± SMS<br/>Twilio API]
    end

    A --> D
    B --> D
    C --> D

    D --> E
    E --> F
    E --> P

    F --> G1
    F --> G2
    F --> G3
    F --> G4
    F --> G5
    F --> G6

    G1 --> H
    G2 --> H
    G3 --> H
    G4 --> H
    G5 --> H
    G6 --> H

    H --> I
    I --> J
    J --> K
    J --> Q

    K --> N
    K --> O

    K --> L
    L --> M

    M --> R
    M --> S
    M --> T

    style D fill:#E3F2FD
    style F fill:#FFF3E0
    style H fill:#FFF9C4
    style L fill:#FFEBEE
    style M fill:#E8F5E9
```

---

## Data Flow Diagram

```mermaid
flowchart TB
    Start([â° Scheduled Run Starts]) --> LoadInput[ğŸ“¥ Load User Input<br/>from Apify Console]
    LoadInput --> LoadWatchlist[ğŸ“‹ Load Watchlist<br/>from KV Store]
    LoadWatchlist --> ValidateInput{âœ… Valid Input?}

    ValidateInput -->|No| ErrorOut[âŒ Log Error<br/>& Exit]
    ValidateInput -->|Yes| ProcessWatchlist[ğŸ”„ Process Each<br/>Watchlist Item]

    ProcessWatchlist --> GenerateQueries[ğŸ” Generate Search<br/>Queries & Variants]
    GenerateQueries --> ParallelScrape[âš¡ Launch 6 Parallel<br/>Platform Scrapers]

    ParallelScrape --> StockX[ğŸª StockX<br/>Sub-Actor]
    ParallelScrape --> GOAT[ğŸª GOAT<br/>Sub-Actor]
    ParallelScrape --> eBay[ğŸª eBay<br/>Sub-Actor]
    ParallelScrape --> Facebook[ğŸª Facebook<br/>Sub-Actor]
    ParallelScrape --> Craigslist[ğŸª Craigslist<br/>Sub-Actor]
    ParallelScrape --> OfferUp[ğŸª OfferUp<br/>Sub-Actor]

    StockX --> Aggregate
    GOAT --> Aggregate
    eBay --> Aggregate
    Facebook --> Aggregate
    Craigslist --> Aggregate
    OfferUp --> Aggregate

    Aggregate[ğŸ”„ Aggregate All<br/>Results] --> Normalize[âœ¨ Normalize Data<br/>Size, Price, SKU]
    Normalize --> Enrich[ğŸ¨ Enrich Data<br/>Colorway, Brand]

    Enrich --> Dedupe[ğŸ” Check Duplicates<br/>Against Seen Listings]
    Dedupe --> CheckSeen{Already Seen?}

    CheckSeen -->|Yes| Skip[â­ï¸ Skip Listing]
    CheckSeen -->|No| NewListing[âœ¨ New Listing Found]

    NewListing --> MarkSeen[âœ… Mark as Seen<br/>in KV Store]
    MarkSeen --> SaveCurrent[ğŸ’¾ Save to<br/>Current Listings Dataset]

    SaveCurrent --> CheckHistorical{ğŸ“Š 24h Since Last<br/>Snapshot?}
    CheckHistorical -->|Yes| SaveHistorical[ğŸ’¾ Save to<br/>Historical Dataset]
    CheckHistorical -->|No| SkipHistorical[â­ï¸ Skip Snapshot]

    SaveHistorical --> EvaluateAlert
    SkipHistorical --> EvaluateAlert

    EvaluateAlert[ğŸš¨ Evaluate Alert<br/>Conditions] --> CheckConditions{ğŸ¯ Matches Watchlist<br/>Conditions?}

    CheckConditions -->|No| NoAlert[â­ï¸ No Alert]
    CheckConditions -->|Yes| FlagForAlert[ğŸ”” Flag for Alert]

    FlagForAlert --> QueueAlert[ğŸ“‹ Add to Alert Queue]
    QueueAlert --> AllProcessed{All Listings<br/>Processed?}

    NoAlert --> AllProcessed
    Skip --> AllProcessed

    AllProcessed -->|No| ProcessWatchlist
    AllProcessed -->|Yes| GroupAlerts[ğŸ‘¥ Group Alerts<br/>by User]

    GroupAlerts --> CheckFreq{ğŸ“¬ Check Notification<br/>Frequency}

    CheckFreq -->|Immediate| SendImmediate[âœ‰ï¸ Send Individual<br/>Email per Listing]
    CheckFreq -->|Hourly Digest| SendDigest[ğŸ“¦ Send Hourly<br/>Digest Email]
    CheckFreq -->|Daily| QueueDaily[â° Queue for Daily<br/>Summary]

    SendImmediate --> CheckSlack{ğŸ’¬ Slack Enabled?}
    SendDigest --> CheckSlack
    QueueDaily --> CheckSlack

    CheckSlack -->|Yes| SendSlack[ğŸ’¬ Send Slack<br/>Notification]
    CheckSlack -->|No| CheckSMS

    SendSlack --> CheckSMS{ğŸ“± SMS Enabled?}
    CheckSMS -->|Yes| SendSMS[ğŸ“± Send SMS<br/>via Twilio]
    CheckSMS -->|No| LogMetrics

    SendSMS --> LogMetrics[ğŸ“Š Log Run Metrics<br/>& Statistics]
    LogMetrics --> OutputResults[ğŸ“¤ Push Results to<br/>Actor Output]
    OutputResults --> Complete([âœ… Run Complete])

    ErrorOut --> Complete

    style Start fill:#4CAF50,color:#fff
    style Complete fill:#4CAF50,color:#fff
    style ErrorOut fill:#F44336,color:#fff
    style CheckConditions fill:#FFC107
    style FlagForAlert fill:#FF5722,color:#fff
    style SendImmediate fill:#2196F3,color:#fff
    style SendDigest fill:#2196F3,color:#fff
    style SendSlack fill:#9C27B0,color:#fff
    style SendSMS fill:#E91E63,color:#fff
```

---

## Marketplace Integration Flow

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ User
    participant Actor as ğŸ¯ Main Actor
    participant WM as ğŸ“‹ Watchlist Manager
    participant Orch as ğŸ”„ Orchestrator
    participant StockX as ğŸª StockX Actor
    participant GOAT as ğŸª GOAT Actor
    participant eBay as ğŸª eBay Actor
    participant Proc as âœ¨ Processor
    participant KV as ğŸ”‘ KV Store
    participant DS as ğŸ“¦ Dataset
    participant Email as âœ‰ï¸ Email Service

    User->>Actor: ğŸš€ Trigger Run (Scheduled/Manual)
    Actor->>WM: Load Watchlist Configuration
    WM->>KV: Fetch watchlist-{userId}.json
    KV-->>WM: Return Watchlist Items

    WM->>Actor: Watchlist Ready
    Actor->>Orch: Start Parallel Scraping

    par Parallel Platform Scraping
        Orch->>StockX: scrapeStockX(query, sizes)
        StockX-->>Orch: Return StockX Listings
    and
        Orch->>GOAT: scrapeGOAT(query, sizes)
        GOAT-->>Orch: Return GOAT Listings
    and
        Orch->>eBay: scrapeEbay(query, sizes)
        eBay-->>Orch: Return eBay Listings
    end

    Orch->>Proc: Aggregate Raw Listings
    Proc->>Proc: Normalize Sizes (US/UK/EU)
    Proc->>Proc: Enrich SKUs & Colorways

    Proc->>KV: Check Seen Listings
    KV-->>Proc: Return Seen Hashes

    Proc->>Proc: Filter New Listings Only
    Proc->>KV: Mark New Listings as Seen

    Proc->>DS: Save to Current Listings Dataset
    Proc->>DS: Update Historical Snapshots

    Proc->>Actor: Return Processed Listings
    Actor->>Actor: Evaluate Alert Conditions

    Actor->>Email: Send Alert Email
    Email-->>User: ğŸ”¥ New Sneaker Alert!

    Actor->>DS: Push Final Output
    Actor-->>User: âœ… Run Complete
```

---

## Deduplication Strategy

```mermaid
flowchart LR
    subgraph "Layer 1: URL-Based Deduplication"
        L1A[ğŸ“„ New Listing] --> L1B[Generate Hash<br/>platform + URL + size]
        L1B --> L1C{Hash Exists in<br/>KV Store?}
        L1C -->|Yes| L1D[â­ï¸ Skip Duplicate]
        L1C -->|No| L1E[âœ… Pass to Layer 2]
    end

    subgraph "Layer 2: SKU-Based Cross-Platform"
        L1E --> L2A[Extract SKU<br/>from Listing]
        L2A --> L2B{SKU Available?}
        L2B -->|No| L2E[âœ… Pass to Layer 3]
        L2B -->|Yes| L2C{Same SKU + Size<br/>Already Seen?}
        L2C -->|Yes| L2D[ğŸ”„ Keep Lowest Price<br/>Version Only]
        L2C -->|No| L2E
    end

    subgraph "Layer 3: Fuzzy Title Matching"
        L2E --> L3A[Calculate Levenshtein<br/>Similarity]
        L2D --> L3A
        L3A --> L3B{Similarity > 85%<br/>AND Same Size<br/>AND Price Â±$10?}
        L3B -->|Yes| L3C[âš ï¸ Likely Duplicate<br/>Flag for Review]
        L3B -->|No| L3D[âœ… Unique Listing]
    end

    L3D --> Output[ğŸ’¾ Store in Dataset]
    L3C --> Output
    L1D --> End([End])

    style L1D fill:#FFCDD2
    style L2D fill:#FFF9C4
    style L3C fill:#FFE0B2
    style L3D fill:#C8E6C9
    style Output fill:#B3E5FC
```

---

## Historical Data Tracking

```mermaid
graph TB
    subgraph "Daily Snapshot Process"
        A[â° Scheduled Run<br/>Completes] --> B{24h Since Last<br/>Snapshot for SKU?}
        B -->|No| C[â­ï¸ Skip Snapshot]
        B -->|Yes| D[ğŸ“Š Calculate Daily Stats]

        D --> E[ğŸ“ˆ Compute Average Price<br/>Lowest & Highest]
        E --> F[ğŸ“¦ Count Active Listings]
        F --> G[ğŸ’¾ Write to Historical<br/>Dataset]

        G --> H[(Historical Snapshots<br/>SKU + Size + Platform + Date)]
    end

    subgraph "Trend Calculation (On Query)"
        I[ğŸ“Š User Requests<br/>Historical Data] --> J[ğŸ” Query Last 30/90/365<br/>Days from Dataset]
        J --> K[ğŸ“‰ Calculate % Changes<br/>1d, 7d, 30d, 90d]
        K --> L[ğŸ“Š Compute Volatility<br/>Standard Deviation]
        L --> M[ğŸ“ˆ Determine Trend<br/>Upward/Downward/Stable]
        M --> N[ğŸ¯ Generate Insights<br/>Buy/Sell Signals]
        N --> O[ğŸ“¤ Return to User]
    end

    H -.->|Read Historical Data| J

    style A fill:#E3F2FD
    style G fill:#C5E1A5
    style H fill:#FFF9C4
    style I fill:#F8BBD0
    style N fill:#CE93D8
```

---

## Alert Evaluation Logic

```mermaid
flowchart TD
    Start([New Listing Found]) --> CheckModel{Model Name<br/>Matches?}

    CheckModel -->|No| NoAlert[âŒ No Alert]
    CheckModel -->|Yes| CheckSize{Size in<br/>Watchlist?}

    CheckSize -->|No| NoAlert
    CheckSize -->|Yes| CheckPrice{Price â‰¤<br/>maxPrice?}

    CheckPrice -->|No| NoAlert
    CheckPrice -->|Yes| CheckCondition{Condition â‰¥<br/>minCondition?}

    CheckCondition -->|No| NoAlert
    CheckCondition -->|Yes| CheckPlatform{Platform in<br/>Allowed List?}

    CheckPlatform -->|No| NoAlert
    CheckPlatform -->|Yes| CheckLocation{Location<br/>Filter Set?}

    CheckLocation -->|No| CheckAuth
    CheckLocation -->|Yes| WithinLocation{Within Target<br/>Location?}

    WithinLocation -->|No| NoAlert
    WithinLocation -->|Yes| CheckAuth{Auth-Only<br/>Filter Enabled?}

    CheckAuth -->|No| Trigger
    CheckAuth -->|Yes| IsAuth{Authenticity<br/>Status = Verified?}

    IsAuth -->|No| NoAlert
    IsAuth -->|Yes| Trigger[ğŸ”” Trigger Alert!]

    Trigger --> CalculateDeal[ğŸ’° Calculate Deal Score<br/>vs Market Average]
    CalculateDeal --> QueueNotification[ğŸ“‹ Add to<br/>Notification Queue]
    QueueNotification --> End([âœ… Continue])

    NoAlert --> End

    style Start fill:#4CAF50,color:#fff
    style NoAlert fill:#F44336,color:#fff
    style Trigger fill:#FF9800,color:#fff
    style QueueNotification fill:#2196F3,color:#fff
    style End fill:#9E9E9E,color:#fff
```

---

## Notification Dispatch Flow

```mermaid
graph TB
    subgraph "Alert Queue Processing"
        A[ğŸ“‹ Alert Queue<br/>Multiple Listings] --> B[ğŸ‘¥ Group by User]
        B --> C[ğŸ“Š Sort by Deal Score<br/>Best Deals First]
    end

    subgraph "Email Notifications"
        C --> D{Email<br/>Frequency?}
        D -->|Immediate| E[âœ‰ï¸ Send Individual<br/>Emails max 5]
        D -->|Hourly| F[ğŸ“¦ Send Digest<br/>All Listings]
        D -->|Daily| G[â° Queue for<br/>Daily Summary]

        E --> H[ğŸ“§ SendGrid API]
        F --> H
        G --> I[(Queue Store<br/>Pending Daily)]
    end

    subgraph "Slack Notifications"
        C --> J{Slack<br/>Enabled?}
        J -->|Yes| K[ğŸ’¬ Format Slack<br/>Rich Blocks]
        J -->|No| N
        K --> L[ğŸ”„ Rate Limit<br/>1 msg/second]
        L --> M[ğŸŒ POST to<br/>Webhook URL]
    end

    subgraph "SMS Notifications (Premium)"
        C --> N{SMS<br/>Enabled?}
        N -->|Yes| O[ğŸ“± Select TOP 1<br/>Best Deal Only]
        N -->|No| S
        O --> P[ğŸ“ Format 160 char<br/>Message]
        P --> Q[ğŸ’° Check Daily<br/>SMS Quota]
        Q --> R[ğŸ“² Twilio API]
    end

    H --> S[ğŸ“Š Log Alert History]
    M --> S
    R --> S
    I -.->|Next Day 8am| F

    S --> T[(Alert History<br/>Dataset)]
    T --> U[âœ… Dispatch Complete]

    style A fill:#FFF3E0
    style H fill:#2196F3,color:#fff
    style M fill:#9C27B0,color:#fff
    style R fill:#E91E63,color:#fff
    style S fill:#4CAF50,color:#fff
```

---

## Portfolio Tracking System

```mermaid
flowchart LR
    subgraph "User Portfolio Input"
        A[ğŸ‘¤ User Provides<br/>Collection List] --> B[ğŸ“ Portfolio Items<br/>Model + Size + Purchase Price]
        B --> C[ğŸ’¾ Store in<br/>KV Store]
    end

    subgraph "Valuation Process"
        D[â° Scheduled Run] --> E[ğŸ” Load User Portfolio<br/>from KV Store]
        E --> F[ğŸ”„ For Each Item]

        F --> G[ğŸ” Query Current<br/>Market Price]
        G --> H[ğŸ“Š Check Historical<br/>Data]

        H --> I[ğŸ’µ Calculate<br/>Current Value]
        I --> J[ğŸ“ˆ Calculate ROI<br/>profit - purchasePrice]

        J --> K{More Items?}
        K -->|Yes| F
        K -->|No| L[ğŸ“Š Aggregate<br/>Portfolio Metrics]
    end

    subgraph "Portfolio Analytics"
        L --> M[ğŸ’° Total Market Value]
        L --> N[ğŸ“ˆ Total ROI %]
        L --> O[ğŸ† Top Gainers<br/>Top 3 by ROI]
        L --> P[ğŸ“‰ Top Losers<br/>Bottom 3 by ROI]

        M --> Q[ğŸ“¤ Output Portfolio<br/>Summary]
        N --> Q
        O --> Q
        P --> Q
    end

    C -.->|Read Portfolio| E
    Q --> R[âœ‰ï¸ Optional: Email<br/>Weekly Report]

    style A fill:#E1F5FE
    style C fill:#FFF9C4
    style I fill:#C8E6C9
    style Q fill:#F8BBD0
    style R fill:#D1C4E9
```

---

## Error Handling & Retry Strategy

```mermaid
stateDiagram-v2
    [*] --> Attempt1: Start Platform Scrape

    Attempt1 --> Success: âœ… Success
    Attempt1 --> Retry2: âŒ Error (Network/Blocking)

    Retry2 --> Wait5s: Exponential Backoff
    Wait5s --> Attempt2: Retry with Delay

    Attempt2 --> Success: âœ… Success
    Attempt2 --> Retry3: âŒ Error Again

    Retry3 --> Wait10s: Exponential Backoff
    Wait10s --> Attempt3: Final Retry

    Attempt3 --> Success: âœ… Success
    Attempt3 --> GracefulFail: âŒ All Attempts Failed

    Success --> [*]: Return Data

    GracefulFail --> LogError: ğŸ“ Log Detailed Error
    LogError --> SwitchProxy: ğŸ”„ Switch to Residential Proxy
    SwitchProxy --> NotifyAdmin: ğŸ“§ Notify if Persistent
    NotifyAdmin --> ContinueOther: â­ï¸ Continue with Other Platforms
    ContinueOther --> [*]: Partial Success

    note right of Attempt1
        Try 1: Use configured proxy
        Timeout: 30s
    end note

    note right of Attempt2
        Try 2: Wait 5s
        Switch proxy pool
    end note

    note right of Attempt3
        Try 3: Wait 10s
        Escalate to residential
    end note

    note right of GracefulFail
        Don't fail entire run
        Log error for user
        Continue with other platforms
    end note
```

---

## Apify Platform Integration

```mermaid
graph TB
    subgraph "Apify Platform Services"
        A[ğŸ¯ Sneaker Collector Actor] --> B[ğŸ“¦ Actor Storage]
        A --> C[ğŸ”‘ Key-Value Store]
        A --> D[ğŸ“Š Dataset Storage]
        A --> E[â° Scheduler]
        A --> F[ğŸŒ Apify Proxy]
        A --> G[ğŸ“§ Actor Integrations]

        B --> B1[(Docker Image<br/>Node.js + Code)]
        C --> C1[(KV: Watchlists)]
        C --> C2[(KV: Seen Listings)]
        D --> D1[(DS: Current Listings)]
        D --> D2[(DS: Historical Data)]
        E --> E1[â° Cron Scheduler<br/>Hourly/30min/15min]
        F --> F1[ğŸ¢ Datacenter Proxies]
        F --> F2[ğŸ  Residential Proxies]
        G --> G1[âœ‰ï¸ Email Notifications]
        G --> G2[ğŸ’¬ Slack Webhooks]
    end

    subgraph "Sub-Actor Orchestration"
        A --> SA[ğŸ”„ Call Sub-Actors<br/>Actor.call()]
        SA --> SA1[ecomscrape/stockx-scraper]
        SA --> SA2[ecomscrape/goat-scraper]
        SA --> SA3[apify/ebay-scraper]
        SA --> SA4[apify/facebook-marketplace]
        SA --> SA5[zscrape/craigslist-scraper]
    end

    subgraph "User Interface"
        UI1[ğŸ–¥ï¸ Apify Console] --> E
        UI1 --> H[â–¶ï¸ Manual Run]
        UI1 --> I[ğŸ“Š View Results]
        UI2[ğŸ”Œ Apify API] --> J[ğŸ¤– External Automation]
    end

    H --> A
    J --> A
    I --> D

    style A fill:#4CAF50,color:#fff
    style B fill:#2196F3,color:#fff
    style C fill:#FF9800,color:#fff
    style D fill:#9C27B0,color:#fff
    style E fill:#F44336,color:#fff
    style F fill:#00BCD4,color:#fff
    style G fill:#8BC34A,color:#fff
```

---

## Challenge Success Pathway

```mermaid
journey
    title Apify $1M Challenge Journey (Nov 2025 - Jan 2026)
    section Week 1-2: MVP Development
        Scaffold Actor: 3: Developer
        Integrate StockX/GOAT: 4: Developer
        Add Email Alerts: 5: Developer
        Write README: 4: Developer
        Publish to Store: 5: Developer, Users
    section Week 3-4: Platform Expansion
        Add Facebook/Craigslist: 4: Developer
        Launch Marketing: 5: Developer, Users
        First 50 Users: 5: Users
        YouTube Tutorial: 5: Developer, Users
    section Week 5-8: Feature Enhancement
        Historical Data: 5: Developer
        Portfolio Tracking: 5: Developer, Users
        Reach 200 MAUs: 5: Users
        Quality Score 75+: 5: Developer
    section Week 9-11: MAU Maximization
        Viral Marketing: 5: Developer, Users
        Discord Partnerships: 5: Users
        500-1000 MAUs: 5: Users
        Grand Prize Consideration: 5: Developer, Users
```

---

## Data Schema Relationships

```mermaid
erDiagram
    USER ||--o{ WATCHLIST_ITEM : "creates"
    USER ||--o{ PORTFOLIO_ITEM : "owns"
    USER ||--|| NOTIFICATION_CONFIG : "has"

    WATCHLIST_ITEM ||--o{ LISTING : "matches"
    WATCHLIST_ITEM {
        string id PK
        string model
        array sizes
        number maxPrice
        string minCondition
        array platforms
        datetime createdAt
        boolean active
    }

    LISTING ||--o{ ALERT_HISTORY : "triggers"
    LISTING {
        string id PK
        string platform
        string url
        string model
        string sku
        string size
        number price
        string condition
        string authenticityStatus
        datetime scrapedDate
        boolean alertTriggered
    }

    LISTING ||--o{ HISTORICAL_SNAPSHOT : "generates"
    HISTORICAL_SNAPSHOT {
        string id PK
        date date
        string sku
        string size
        string platform
        number price
        number listingsCount
        number priceChange30d
        string trend
    }

    PORTFOLIO_ITEM ||--o| LISTING : "valued_by"
    PORTFOLIO_ITEM {
        string id PK
        string model
        string size
        number purchasePrice
        date purchaseDate
        number currentMarketValue
        number roi
    }

    NOTIFICATION_CONFIG {
        string userId PK
        object email
        object slack
        object sms
    }

    ALERT_HISTORY {
        string id PK
        datetime timestamp
        string userId FK
        string watchlistItemId FK
        string listingId FK
        string channel
        string status
    }
```

---

## Proxy Strategy Decision Tree

```mermaid
flowchart TD
    Start([Platform Scrape Request]) --> CheckPlatform{Which Platform?}

    CheckPlatform -->|StockX| UseRes1[Use Residential Proxy]
    CheckPlatform -->|GOAT| UseRes2[Use Residential Proxy]
    CheckPlatform -->|Facebook| UseRes3[Use Residential Proxy]
    CheckPlatform -->|OfferUp| UseRes4[Use Residential Proxy]
    CheckPlatform -->|eBay| CheckFailures1{Previous Failures?}
    CheckPlatform -->|Craigslist| CheckFailures2{Previous Failures?}

    CheckFailures1 -->|<2| UseDC1[Use Datacenter Proxy]
    CheckFailures1 -->|â‰¥2| UseRes5[Escalate to Residential]

    CheckFailures2 -->|<2| UseDC2[Use Datacenter Proxy]
    CheckFailures2 -->|â‰¥2| UseRes6[Escalate to Residential]

    UseRes1 --> SetConfig1[Set apifyProxyGroups: RESIDENTIAL<br/>Country: US]
    UseRes2 --> SetConfig1
    UseRes3 --> SetConfig1
    UseRes4 --> SetConfig1
    UseRes5 --> SetConfig1
    UseRes6 --> SetConfig1

    UseDC1 --> SetConfig2[Set apifyProxyGroups: GOOGLE_SERP<br/>Country: US]
    UseDC2 --> SetConfig2

    SetConfig1 --> ExecuteScrape[Execute Scrape Request]
    SetConfig2 --> ExecuteScrape

    ExecuteScrape --> CheckResult{Success?}

    CheckResult -->|Yes| Success[âœ… Return Data<br/>Cost: $0.20/1K residential<br/>or $0.01/1K datacenter]
    CheckResult -->|No| IncrementFail[Increment Failure Counter]

    IncrementFail --> CheckRetries{Retries Left?}
    CheckRetries -->|Yes| Start
    CheckRetries -->|No| Fail[âŒ Log Error<br/>Continue with Other Platforms]

    Success --> End([End])
    Fail --> End

    style UseRes1 fill:#FF6B6B
    style UseRes2 fill:#FF6B6B
    style UseRes3 fill:#FF6B6B
    style UseRes4 fill:#FF6B6B
    style UseDC1 fill:#4ECDC4
    style UseDC2 fill:#4ECDC4
    style Success fill:#95E1D3
    style Fail fill:#F38181
```

---

## Complete System Context Diagram

```mermaid
C4Context
    title System Context - Sneaker Collector's Assistant

    Person(user, "Sneaker Collector/Reseller", "Tracks rare sneakers<br/>across marketplaces")

    System(actor, "Sneaker Collector's Assistant", "Apify Actor that aggregates<br/>sneaker listings and sends alerts")

    System_Ext(stockx, "StockX", "Authenticated sneaker<br/>resale marketplace")
    System_Ext(goat, "GOAT", "Authenticated sneaker<br/>resale marketplace")
    System_Ext(ebay, "eBay", "Global auction and<br/>retail marketplace")
    System_Ext(facebook, "Facebook Marketplace", "Local classified ads<br/>marketplace")
    System_Ext(craigslist, "Craigslist", "Local classified ads<br/>platform")
    System_Ext(offerup, "OfferUp", "Local buying and<br/>selling platform")

    System_Ext(sendgrid, "SendGrid", "Email delivery service")
    System_Ext(slack, "Slack", "Team communication<br/>platform")
    System_Ext(twilio, "Twilio", "SMS messaging service")

    System_Ext(apify, "Apify Platform", "Serverless automation<br/>infrastructure")

    Rel(user, actor, "Configures watchlist &<br/>receives alerts", "Apify Console/API")

    Rel(actor, stockx, "Scrapes listings", "HTTP/HTTPS via proxy")
    Rel(actor, goat, "Scrapes listings", "HTTP/HTTPS via proxy")
    Rel(actor, ebay, "Scrapes listings", "HTTP/HTTPS via proxy")
    Rel(actor, facebook, "Scrapes listings", "HTTP/HTTPS via proxy")
    Rel(actor, craigslist, "Scrapes listings", "HTTP/HTTPS via proxy")
    Rel(actor, offerup, "Scrapes listings", "HTTP/HTTPS via proxy")

    Rel(actor, sendgrid, "Sends email alerts", "SendGrid API")
    Rel(actor, slack, "Posts notifications", "Webhook")
    Rel(actor, twilio, "Sends SMS alerts", "Twilio API")

    Rel(actor, apify, "Runs on, stores data", "Apify SDK")
```

---

## Marketing Funnel Visualization

```mermaid
graph TB
    subgraph "Awareness Stage"
        A1[Reddit Posts<br/>r/Sneakers 3.8M] --> B[50,000 Views]
        A2[YouTube Tutorial<br/>Sneaker Automation] --> B
        A3[Discord Cook Groups<br/>100+ Servers] --> B
        A4[Twitter/X Posts<br/>Sneaker Community] --> B
    end

    subgraph "Interest Stage"
        B --> C[5,000 Click-Throughs<br/>10% CTR]
        C --> D[Visit Apify Store Page]
        D --> E{Compelling README<br/>+ Demo Video?}
        E -->|Yes| F[2,000 Users Interested<br/>40% Conversion]
        E -->|No| G[âŒ Bounce]
    end

    subgraph "Activation Stage"
        F --> H[Create Apify Account]
        H --> I[Configure First Watchlist]
        I --> J{Setup Complete<br/>in <5 minutes?}
        J -->|Yes| K[1,200 Activated Users<br/>60% Activation]
        J -->|No| L[âŒ Abandon]
    end

    subgraph "Engagement Stage"
        K --> M{Receive First Alert<br/>within 24h?}
        M -->|Yes| N[800 Engaged Users<br/>67% Engagement]
        M -->|No| O[âŒ Inactive]
        N --> P[Schedule Hourly Runs]
    end

    subgraph "Retention Stage"
        P --> Q{Still Active<br/>After 30 Days?}
        Q -->|Yes| R[600 Monthly Active Users<br/>75% Retention]
        Q -->|No| S[âŒ Churned]
        R --> T[ğŸ¯ Challenge MAU Target]
    end

    subgraph "Revenue Stage (Post-Challenge)"
        R --> U{Upgrade to Paid?}
        U -->|Yes| V[60-120 Paid Users<br/>10-20% Conversion]
        U -->|No| W[Continue Free Tier]
        V --> X[$540-$2,280 MRR]
    end

    style B fill:#FFF9C4
    style F fill:#C5E1A5
    style K fill:#90CAF9
    style N fill:#CE93D8
    style R fill:#A5D6A7
    style V fill:#81C784
    style X fill:#66BB6A

    style G fill:#FFCDD2
    style L fill:#FFCDD2
    style O fill:#FFCDD2
    style S fill:#FFCDD2
```

---

## Technology Stack Overview

```mermaid
mindmap
  root((Sneaker Actor<br/>Tech Stack))
    Runtime
      Apify SDK v3
      Node.js 20
      TypeScript 5
      Docker Container

    Storage
      Apify Dataset
        Current Listings
        Historical Snapshots
      Apify KV Store
        User Watchlists
        Seen Listings
        Notification Queue

    Scrapers
      Existing Actors
        ecomscrape/stockx-scraper
        ecomscrape/goat-scraper
        apify/ebay-scraper
        apify/facebook-marketplace
        zscrape/craigslist-scraper
      Custom Integration
        OfferUp Wrapper

    Networking
      Apify Proxy
        Residential Proxies
        Datacenter Proxies
      Anti-Blocking
        Rotating IPs
        User-Agent Rotation
        Rate Limiting

    Notifications
      Email
        SendGrid API
        HTML Templates
        Digest Batching
      Chat
        Slack Webhooks
        Rich Message Blocks
      SMS
        Twilio API
        160 char messages

    Data Processing
      Normalization
        Size Conversion US/UK/EU
        Price Currency Standardization
      Enrichment
        SKU Extraction
        Colorway Detection
      Deduplication
        SHA-256 Hashing
        Fuzzy Matching
      Analytics
        Historical Trends
        Deal Scoring
        ROI Calculation

    Scheduling
      Apify Scheduler
        Cron Expression
        Timezone Support
      Retry Logic
        Exponential Backoff
        Graceful Degradation
```

---

## Performance Optimization Strategy

```mermaid
flowchart TD
    Start([Actor Run Starts]) --> A1[Sequential Processing<br/>â±ï¸ 180s total]
    Start --> A2[âŒ Large Watchlist<br/>20 items Ã— 6 platforms = 120 calls]
    Start --> A3[âŒ Slow Historical Queries<br/>365 days Ã— 100 SKUs]

    A1 --> B1[âœ… Parallel Processing<br/>Promise.all 6 platforms<br/>â±ï¸ 30s max]
    A2 --> B2[âœ… Batch Processing<br/>Process 5 items at a time<br/>Reduce memory pressure]
    A3 --> B3[âœ… In-Memory Caching<br/>Cache historical data per run<br/>Avoid redundant queries]

    B1 --> C1[ğŸ’° Cost: Same<br/>âš¡ Speed: 6x faster]
    B2 --> C2[ğŸ’° Cost: Same<br/>ğŸ§  Memory: 60% reduction]
    B3 --> C3[ğŸ’° Cost: -40%<br/>âš¡ Speed: 5x faster]

    C1 --> D[Combined Optimizations]
    C2 --> D
    C3 --> D

    D --> E[Original: 180s, $1.50, 3GB RAM]
    D --> F[Optimized: 45s, $0.90, 1.2GB RAM]

    F --> G[âœ… 4x Faster<br/>âœ… 40% Cheaper<br/>âœ… 60% Less Memory]

    style A1 fill:#FFCDD2
    style A2 fill:#FFCDD2
    style A3 fill:#FFCDD2
    style B1 fill:#C8E6C9
    style B2 fill:#C8E6C9
    style B3 fill:#C8E6C9
    style G fill:#4CAF50,color:#fff
```

---

These diagrams provide comprehensive visual documentation of the Sneaker Collector's Assistant
architecture, covering:

1. âœ… System Architecture (high-level overview)
2. âœ… Data Flow (detailed step-by-step process)
3. âœ… Marketplace Integration (sequence diagram)
4. âœ… Deduplication Strategy (3-layer approach)
5. âœ… Historical Data Tracking
6. âœ… Alert Evaluation Logic
7. âœ… Notification Dispatch Flow
8. âœ… Portfolio Tracking System
9. âœ… Error Handling & Retry Strategy
10. âœ… Apify Platform Integration
11. âœ… Challenge Success Pathway
12. âœ… Data Schema Relationships (ERD)
13. âœ… Proxy Strategy Decision Tree
14. âœ… System Context (C4 Model)
15. âœ… Marketing Funnel
16. âœ… Technology Stack (Mind Map)
17. âœ… Performance Optimization

You can render these Mermaid diagrams using:

- GitHub/GitLab (native Mermaid support)
- Mermaid Live Editor (https://mermaid.live)
- VS Code with Mermaid extension
- Documentation tools (MkDocs, Docusaurus)

All diagrams can be embedded directly in Markdown documents and will render automatically on most
modern documentation platforms.
